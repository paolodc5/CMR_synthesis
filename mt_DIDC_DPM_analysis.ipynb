{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7999d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"  \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from diffusers import DDPMScheduler, UNet2DModel, get_cosine_schedule_with_warmup\n",
    "from datasets import RAMDatasetDIDC, LazyDatasetDIDC\n",
    "from mt_DIDC_config import GROUPING_RULES, NEW_LABELS\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa74002",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'New_dictionary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8a55cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE = (128, 128)\n",
    "N_EPOCHS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bfcbf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lazy Dataset: File...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing files and slices:  98%|█████████▊| 480/489 [00:11<00:00, 36.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5645 samples indexed.\n",
      "Dataset size: 5645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing files and slices: 100%|██████████| 489/489 [00:11<00:00, 41.30it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = LazyDatasetDIDC(DATA_PATH, grouping_rules=GROUPING_RULES, new_labels=NEW_LABELS)\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "818417fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['input_label', 'multiClassMask'])\n",
      "Batch fg shape: torch.Size([2, 4, 384, 384])\n",
      "Batch mask shape: torch.Size([2, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"Batch keys: {batch.keys()}\")\n",
    "print(f\"Batch fg shape: {batch['input_label'].shape}\")\n",
    "print(f\"Batch mask shape: {batch['multiClassMask'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47aa1918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_label'].dtype, batch['multiClassMask'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1756ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch, model, num_classes, optimizer, noise_scheduler, accelerator):\n",
    "    model\n",
    "    model.train()\n",
    "\n",
    "    clean_images = batch['multiClassMask']  # Shape: (B, C, H, W)\n",
    "    clean_images = F.one_hot(clean_images.long(), num_classes=num_classes).permute(0, 3, 1, 2).float()  # Shape: (B, C, H, W)\n",
    "    clean_images = clean_images * 2.0 - 1.0  # Scale to [-1, 1]\n",
    "\n",
    "    batch_size = clean_images.size(0)\n",
    "    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=accelerator.device).long()\n",
    "\n",
    "    noise = torch.randn_like(clean_images.float())\n",
    "    noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "    noise_pred = model(noisy_images, timesteps).sample\n",
    "    loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "    with accelerator.accumulate(model):\n",
    "        optimizer.zero_grad()\n",
    "        accelerator.backward(loss)\n",
    "        if accelerator.sync_gradients:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.detach().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "698c5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator = Accelerator()\n",
    "\n",
    "# # STAMPE DI DEBUG (Il tuo Sanity Check)\n",
    "# print(f\"--- SANITY CHECK ---\")\n",
    "# print(f\"Processo corrente index: {accelerator.process_index}\")\n",
    "# print(f\"Device assegnato a questo processo: {accelerator.device}\")\n",
    "# print(f\"Numero totale di processi (GPU) attivi: {accelerator.num_processes}\")\n",
    "# print(f\"--------------------\\n\")\n",
    "\n",
    "# model, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n",
    "# batch = next(iter(dataloader))\n",
    "\n",
    "# print(f\"[Process {accelerator.process_index}] Il batch si trova su: {batch['multiClassMask'].device}\")\n",
    "# print(f\"[Process {accelerator.process_index}] Il modello si trova su: {next(model.parameters()).device}\")\n",
    "\n",
    "# loss_value = training_step(batch, model, len(dataset.new_labels), optimizer, noise_scheduler, accelerator)\n",
    "\n",
    "# accelerator.print(f\"Step completato con successo! Loss: {loss_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a82540",
   "metadata": {},
   "source": [
    "## Final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d777fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    target_size = 128  # the generated image resolution\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # how many images to sample during evaluation\n",
    "    num_epochs = 50\n",
    "    batch_size_per_gpu = 2\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    gradient_accumulation_steps = max(1, train_batch_size // (batch_size_per_gpu * num_gpus))\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = \"ddpm-butterflies-128\"  # the model name locally and on the HF Hub\n",
    "\n",
    "    seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8eedb17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lazy Dataset: File...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing files and slices:   0%|          | 0/489 [00:00<?, ?it/s]\u001b[A\n",
      "Indexing files and slices:   1%|          | 4/489 [00:00<00:13, 34.66it/s]\u001b[A\n",
      "Indexing files and slices:   2%|▏         | 8/489 [00:00<00:17, 28.07it/s]\u001b[A\n",
      "Indexing files and slices:   2%|▏         | 12/489 [00:00<00:15, 29.96it/s]\u001b[A\n",
      "Indexing files and slices:   4%|▎         | 18/489 [00:00<00:12, 38.01it/s]\u001b[A\n",
      "Indexing files and slices:   5%|▍         | 24/489 [00:00<00:10, 42.60it/s]\u001b[A\n",
      "Indexing files and slices:   6%|▌         | 29/489 [00:00<00:11, 39.77it/s]\u001b[A\n",
      "Indexing files and slices:   7%|▋         | 34/489 [00:00<00:11, 41.28it/s]\u001b[A\n",
      "Indexing files and slices:   8%|▊         | 39/489 [00:00<00:10, 42.90it/s]\u001b[A\n",
      "Indexing files and slices:   9%|▉         | 44/489 [00:01<00:10, 43.72it/s]\u001b[A\n",
      "Indexing files and slices:  10%|█         | 49/489 [00:01<00:10, 42.41it/s]\u001b[A\n",
      "Indexing files and slices:  11%|█         | 54/489 [00:01<00:13, 32.46it/s]\u001b[A\n",
      "Indexing files and slices:  12%|█▏        | 58/489 [00:01<00:13, 31.97it/s]\u001b[A\n",
      "Indexing files and slices:  13%|█▎        | 62/489 [00:01<00:12, 33.07it/s]\u001b[A\n",
      "Indexing files and slices:  13%|█▎        | 66/489 [00:01<00:12, 33.00it/s]\u001b[A\n",
      "Indexing files and slices:  15%|█▍        | 72/489 [00:01<00:11, 37.63it/s]\u001b[A\n",
      "Indexing files and slices:  16%|█▌        | 77/489 [00:02<00:10, 40.56it/s]\u001b[A\n",
      "Indexing files and slices:  17%|█▋        | 82/489 [00:02<00:09, 42.15it/s]\u001b[A\n",
      "Indexing files and slices:  18%|█▊        | 87/489 [00:02<00:09, 43.97it/s]\u001b[A\n",
      "Indexing files and slices:  19%|█▉        | 92/489 [00:02<00:09, 42.75it/s]\u001b[A\n",
      "Indexing files and slices:  20%|█▉        | 97/489 [00:02<00:09, 40.60it/s]\u001b[A\n",
      "Indexing files and slices:  21%|██        | 102/489 [00:02<00:09, 42.61it/s]\u001b[A\n",
      "Indexing files and slices:  22%|██▏       | 107/489 [00:02<00:09, 40.12it/s]\u001b[A\n",
      "Indexing files and slices:  23%|██▎       | 112/489 [00:02<00:08, 42.20it/s]\u001b[A\n",
      "Indexing files and slices:  24%|██▍       | 117/489 [00:02<00:08, 43.75it/s]\u001b[A\n",
      "Indexing files and slices:  25%|██▍       | 122/489 [00:03<00:08, 44.18it/s]\u001b[A\n",
      "Indexing files and slices:  26%|██▌       | 127/489 [00:03<00:07, 45.32it/s]\u001b[A\n",
      "Indexing files and slices:  27%|██▋       | 132/489 [00:03<00:07, 46.31it/s]\u001b[A\n",
      "Indexing files and slices:  28%|██▊       | 137/489 [00:03<00:07, 46.01it/s]\u001b[A\n",
      "Indexing files and slices:  29%|██▉       | 142/489 [00:03<00:08, 43.37it/s]\u001b[A\n",
      "Indexing files and slices:  30%|███       | 147/489 [00:03<00:08, 39.25it/s]\u001b[A\n",
      "Indexing files and slices:  31%|███       | 152/489 [00:03<00:09, 36.17it/s]\u001b[A\n",
      "Indexing files and slices:  32%|███▏      | 157/489 [00:03<00:08, 39.34it/s]\u001b[A\n",
      "Indexing files and slices:  33%|███▎      | 163/489 [00:04<00:07, 42.60it/s]\u001b[A\n",
      "Indexing files and slices:  35%|███▍      | 169/489 [00:04<00:07, 44.67it/s]\u001b[A\n",
      "Indexing files and slices:  36%|███▌      | 174/489 [00:04<00:07, 42.59it/s]\u001b[A\n",
      "Indexing files and slices:  37%|███▋      | 179/489 [00:04<00:07, 43.37it/s]\u001b[A\n",
      "Indexing files and slices:  38%|███▊      | 184/489 [00:04<00:07, 42.24it/s]\u001b[A\n",
      "Indexing files and slices:  39%|███▊      | 189/489 [00:04<00:06, 44.02it/s]\u001b[A\n",
      "Indexing files and slices:  40%|███▉      | 194/489 [00:04<00:07, 38.14it/s]\u001b[A\n",
      "Indexing files and slices:  41%|████      | 199/489 [00:04<00:07, 40.31it/s]\u001b[A\n",
      "Indexing files and slices:  42%|████▏     | 205/489 [00:05<00:06, 43.82it/s]\u001b[A\n",
      "Indexing files and slices:  43%|████▎     | 210/489 [00:05<00:06, 44.70it/s]\u001b[A\n",
      "Indexing files and slices:  44%|████▍     | 215/489 [00:05<00:06, 43.55it/s]\u001b[A\n",
      "Indexing files and slices:  45%|████▍     | 220/489 [00:05<00:06, 43.40it/s]\u001b[A\n",
      "Indexing files and slices:  46%|████▌     | 225/489 [00:05<00:05, 45.04it/s]\u001b[A\n",
      "Indexing files and slices:  47%|████▋     | 230/489 [00:05<00:05, 45.58it/s]\u001b[A\n",
      "Indexing files and slices:  48%|████▊     | 235/489 [00:05<00:05, 46.38it/s]\u001b[A\n",
      "Indexing files and slices:  49%|████▉     | 240/489 [00:05<00:05, 45.93it/s]\u001b[A\n",
      "Indexing files and slices:  50%|█████     | 245/489 [00:05<00:05, 44.76it/s]\u001b[A\n",
      "Indexing files and slices:  51%|█████     | 250/489 [00:06<00:05, 42.72it/s]\u001b[A\n",
      "Indexing files and slices:  52%|█████▏    | 255/489 [00:06<00:05, 44.09it/s]\u001b[A\n",
      "Indexing files and slices:  53%|█████▎    | 260/489 [00:06<00:05, 43.58it/s]\u001b[A\n",
      "Indexing files and slices:  54%|█████▍    | 265/489 [00:06<00:05, 40.39it/s]\u001b[A\n",
      "Indexing files and slices:  55%|█████▌    | 270/489 [00:06<00:06, 35.91it/s]\u001b[A\n",
      "Indexing files and slices:  56%|█████▌    | 274/489 [00:06<00:06, 35.05it/s]\u001b[A\n",
      "Indexing files and slices:  57%|█████▋    | 278/489 [00:06<00:06, 33.19it/s]\u001b[A\n",
      "Indexing files and slices:  58%|█████▊    | 283/489 [00:06<00:05, 35.93it/s]\u001b[A\n",
      "Indexing files and slices:  59%|█████▊    | 287/489 [00:07<00:05, 36.36it/s]\u001b[A\n",
      "Indexing files and slices:  60%|█████▉    | 291/489 [00:07<00:05, 36.73it/s]\u001b[A\n",
      "Indexing files and slices:  60%|██████    | 295/489 [00:07<00:05, 36.33it/s]\u001b[A\n",
      "Indexing files and slices:  61%|██████▏   | 300/489 [00:07<00:05, 37.06it/s]\u001b[A\n",
      "Indexing files and slices:  62%|██████▏   | 304/489 [00:07<00:04, 37.32it/s]\u001b[A\n",
      "Indexing files and slices:  63%|██████▎   | 309/489 [00:07<00:04, 40.70it/s]\u001b[A\n",
      "Indexing files and slices:  64%|██████▍   | 314/489 [00:07<00:04, 43.12it/s]\u001b[A\n",
      "Indexing files and slices:  65%|██████▌   | 319/489 [00:07<00:03, 44.56it/s]\u001b[A\n",
      "Indexing files and slices:  66%|██████▋   | 324/489 [00:07<00:03, 43.51it/s]\u001b[A\n",
      "Indexing files and slices:  67%|██████▋   | 329/489 [00:08<00:03, 42.35it/s]\u001b[A\n",
      "Indexing files and slices:  69%|██████▊   | 335/489 [00:08<00:03, 43.12it/s]\u001b[A\n",
      "Indexing files and slices:  70%|██████▉   | 340/489 [00:08<00:03, 38.16it/s]\u001b[A\n",
      "Indexing files and slices:  70%|███████   | 344/489 [00:08<00:03, 37.58it/s]\u001b[A\n",
      "Indexing files and slices:  71%|███████▏  | 349/489 [00:08<00:03, 40.03it/s]\u001b[A\n",
      "Indexing files and slices:  72%|███████▏  | 354/489 [00:08<00:03, 41.06it/s]\u001b[A\n",
      "Indexing files and slices:  73%|███████▎  | 359/489 [00:08<00:04, 31.64it/s]\u001b[A\n",
      "Indexing files and slices:  74%|███████▍  | 363/489 [00:09<00:04, 30.46it/s]\u001b[A\n",
      "Indexing files and slices:  75%|███████▌  | 367/489 [00:09<00:04, 28.94it/s]\u001b[A\n",
      "Indexing files and slices:  76%|███████▌  | 371/489 [00:09<00:04, 28.02it/s]\u001b[A\n",
      "Indexing files and slices:  77%|███████▋  | 377/489 [00:09<00:03, 33.60it/s]\u001b[A\n",
      "Indexing files and slices:  78%|███████▊  | 382/489 [00:09<00:02, 36.86it/s]\u001b[A\n",
      "Indexing files and slices:  79%|███████▉  | 387/489 [00:09<00:02, 38.46it/s]\u001b[A\n",
      "Indexing files and slices:  80%|████████  | 392/489 [00:09<00:02, 38.53it/s]\u001b[A\n",
      "Indexing files and slices:  81%|████████  | 396/489 [00:10<00:02, 38.19it/s]\u001b[A\n",
      "Indexing files and slices:  82%|████████▏ | 400/489 [00:10<00:02, 36.12it/s]\u001b[A\n",
      "Indexing files and slices:  83%|████████▎ | 405/489 [00:10<00:02, 39.44it/s]\u001b[A\n",
      "Indexing files and slices:  84%|████████▍ | 410/489 [00:10<00:01, 41.38it/s]\u001b[A\n",
      "Indexing files and slices:  85%|████████▍ | 415/489 [00:10<00:01, 42.07it/s]\u001b[A\n",
      "Indexing files and slices:  86%|████████▌ | 420/489 [00:10<00:01, 40.36it/s]\u001b[A\n",
      "Indexing files and slices:  87%|████████▋ | 425/489 [00:10<00:01, 42.53it/s]\u001b[A\n",
      "Indexing files and slices:  88%|████████▊ | 430/489 [00:10<00:01, 42.00it/s]\u001b[A\n",
      "Indexing files and slices:  89%|████████▉ | 435/489 [00:10<00:01, 37.20it/s]\u001b[A\n",
      "Indexing files and slices:  90%|████████▉ | 440/489 [00:11<00:01, 39.02it/s]\u001b[A\n",
      "Indexing files and slices:  91%|█████████ | 445/489 [00:11<00:01, 41.52it/s]\u001b[A\n",
      "Indexing files and slices:  92%|█████████▏| 450/489 [00:11<00:00, 43.60it/s]\u001b[A\n",
      "Indexing files and slices:  93%|█████████▎| 455/489 [00:11<00:00, 45.19it/s]\u001b[A\n",
      "Indexing files and slices:  94%|█████████▍| 460/489 [00:11<00:00, 46.42it/s]\u001b[A\n",
      "Indexing files and slices:  95%|█████████▌| 466/489 [00:11<00:00, 47.80it/s]\u001b[A\n",
      "Indexing files and slices:  96%|█████████▋| 471/489 [00:11<00:00, 47.72it/s]\u001b[A\n",
      "Indexing files and slices:  97%|█████████▋| 476/489 [00:11<00:00, 47.30it/s]\u001b[A\n",
      "Indexing files and slices:  98%|█████████▊| 481/489 [00:11<00:00, 45.19it/s]\u001b[A\n",
      "Indexing files and slices: 100%|██████████| 489/489 [00:12<00:00, 40.31it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5645 samples indexed.\n"
     ]
    }
   ],
   "source": [
    "config = TrainingConfig()\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"sigmoid\")\n",
    "model = UNet2DModel(sample_size=384,  in_channels=22,   out_channels=22, layers_per_block=2,block_out_channels=(128, 256, 512, 512),\n",
    "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    ")\n",
    "\n",
    "dataset = LazyDatasetDIDC(DATA_PATH, grouping_rules=GROUPING_RULES, new_labels=NEW_LABELS)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)\n",
    "\n",
    "model_summary = summary(model, input_data=(torch.randn(1, 22, 384, 384), torch.tensor([0])))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=config.lr_warmup_steps,num_training_steps=(len(train_dataloader) * config.num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71cb84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\"),\n",
    "    )\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        if config.output_dir is not None:\n",
    "            os.makedirs(config.output_dir, exist_ok=True)\n",
    "        accelerator.init_trackers(\"train_example\")\n",
    "\n",
    "    # Prepare accelerator model\n",
    "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(model, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            loss = training_step(batch, model, len(train_dataloader.dataset.new_labels), optimizer, noise_scheduler, accelerator)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss, \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            global_step += 1\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            # implement sampling pipeline\n",
    "\n",
    "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                # implement evaluate() to see how well your model is doing, and optionally save generated images\n",
    "                pass\n",
    "\n",
    "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "                # implement saving logic\n",
    "                pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a899eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/pdiciano/GenAI/ACDC_mine/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:529: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "Epoch 0:   0%|          | 1/2823 [00:04<3:29:37,  4.46s/it]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_last_lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m TrainingConfig()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 27\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m training_step(batch, model, \u001b[38;5;28mlen\u001b[39m(train_dataloader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mnew_labels), optimizer, noise_scheduler, accelerator)\n\u001b[1;32m     26\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_last_lr\u001b[49m()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m: global_step}\n\u001b[1;32m     28\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_postfix(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlogs)\n\u001b[1;32m     29\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mlog(logs, step\u001b[38;5;241m=\u001b[39mglobal_step)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_last_lr'"
     ]
    }
   ],
   "source": [
    "config = TrainingConfig()\n",
    "train_loop(config, model, noise_scheduler, optimizer, dataloader, lr_scheduler=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
